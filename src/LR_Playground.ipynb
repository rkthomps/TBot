{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f28e180-1185-4dcf-8562-b0634b82d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Alpaca_Scraper import Alpaca_Scraper\n",
    "from Company_Lister import Company_Lister\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaaa6e2-7889-40e3-881c-452d4044403a",
   "metadata": {},
   "source": [
    "We can do beta testing with the google stock. This should scale to other stocks. This cell updates the stock data we store for the stocks we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b629792d-5a09-45a5-9bb7-959603f4e609",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/MyProjects/TBot/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MyProjects/TBot/venv/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/MyProjects/TBot/venv/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/9htyqd7s7fvcqcn47w7nfyqc0000gn/T/ipykernel_70400/2370998341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompany_Lister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_snp_since\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2011\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0myesterday_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrelativedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mAlpaca_Scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_tickers_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2011-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myesterday_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MyProjects/TBot/src/Alpaca_Scraper.py\u001b[0m in \u001b[0;36mupdate_tickers_data\u001b[0;34m(self, tickers, f_date, t_date)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_storage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mcurrent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_storage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mcurrent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mdate_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mmax_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MyProjects/TBot/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MyProjects/TBot/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "data_loc = '../data_files/stock_data/'\n",
    "tickers = Company_Lister().get_snp_since(datetime.datetime(2011, 1, 1))\n",
    "yesterday_str = str((datetime.datetime.now() - relativedelta(days=3)).date())\n",
    "Alpaca_Scraper().update_tickers_data(tickers, '2011-01-01', yesterday_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401e4ca-a1a6-45d8-b1c1-9a37aeee185b",
   "metadata": {},
   "source": [
    "Here we read the stock data in for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263aaf3e-66d4-4f44-839a-c226b04f75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = glob.glob(data_loc + '*.csv')\n",
    "stock_dfs = {}\n",
    "for csv in csvs:\n",
    "    symbol = re.search(r'/((\\w|\\.)+)\\.csv', csv).groups()[0]\n",
    "    stock_df = pd.read_csv(csv)\n",
    "    stock_df['timestamp'] = pd.to_datetime(stock_df['timestamp'])\n",
    "    stock_df['date'] = stock_df['timestamp'].dt.date\n",
    "    stock_df = stock_df.drop(stock_df.loc[stock_df['date'].duplicated()].index)\n",
    "    stock_df = stock_df.drop(columns='date', axis=1)\n",
    "    stock_dfs[symbol] = stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04e693-f9ef-4092-9754-46d39abfd1e1",
   "metadata": {},
   "source": [
    "### To estimate the change in stock price after a week, given 8 weeks of data, will will format the input as 8 weeks, or 40 days of stock market data where every input begins on a Monday, and ends on a Friday. That is, each week must be composed of 5 days. If there is a missing value, it will be imputed by the mean of its neighbors. Each of the daily inputs will have\n",
    "* Closing Price\n",
    "* Volume\n",
    "\n",
    "**The input as a whole will be tied to 12 indicator variables that capture the month of the year**\n",
    "\n",
    "### We will start a new input, response pair for every week of stock price data. This creates a rough estimate of 5 years * 52 weeks ~ 250 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723d7d0e-1f4e-4985-ad64-d825355c3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the timestamp of the first monday in the dataset\n",
    "# This function assumes\n",
    "def get_first_monday(stock_df):\n",
    "    #Add weekday (as an index starting with 0 as sunday) to the dataset\n",
    "    stock_df['weekday'] = stock_df['timestamp'].apply(lambda x: int(datetime.datetime.strftime(x, '%w')))\n",
    "    starting_weekday = stock_df.iloc[0, stock_df.columns.get_loc('weekday')]\n",
    "    starting_timestamp = stock_df.iloc[0, stock_df.columns.get_loc('timestamp')]\n",
    "    \n",
    "    #Identify the first monday in the dataset\n",
    "    if starting_weekday <= 1:\n",
    "        return relativedelta(days=1 - int(starting_weekday)) + starting_timestamp\n",
    "    else:\n",
    "        return relativedelta(days=8 - int(starting_weekday)) + starting_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdde3c3-692c-49dc-9f58-2472f1af43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts one of the variables in the dataframe to one hot encoding\n",
    "def to_one_hot(df, var, dummy=False):\n",
    "    series = df[var]\n",
    "    df_wo_var = df.drop(columns=var, axis=1)\n",
    "    var_vals = dict([(v, i) for i, v in enumerate(list(series.unique()))])\n",
    "    rev_var = dict([(i, v) for v, i in var_vals.items()])\n",
    "    var_ind = series.apply(lambda x: var_vals[x])\n",
    "    new_mat = var_ind.values[:, np.newaxis] == np.arange(len(rev_var))[np.newaxis, :]\n",
    "    columns = [rev_var[i] for i in range(len(rev_var))]\n",
    "    new_df = pd.DataFrame(new_mat, columns=columns).astype(int)\n",
    "    if dummy:\n",
    "        new_df.drop(columns=rev_var[len(rev_var) - 1], axis=1)\n",
    "    return pd.concat((df_wo_var, new_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f360420-2aa6-48b1-a2b2-54d98c88938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns every row in the dataframe an index that corresponds to the week it belongs to.\n",
    "# This index is computed from the first monday in the dataframe\n",
    "def assign_week_index(stock_df):\n",
    "    #Identify the first monday in the dataset\n",
    "    first_time = get_first_monday(stock_df)\n",
    "        \n",
    "    #Find each monday to the end of the dataset. Create a dictionary matching the monday\n",
    "    #with an index since the first monday\n",
    "    max_date = stock_df['timestamp'].max()\n",
    "    mondays = [first_time.date()]\n",
    "    while True:\n",
    "        cur_len = len(mondays)\n",
    "        next_date = first_time + relativedelta(days = cur_len * 7)\n",
    "        if next_date > max_date:\n",
    "            break\n",
    "        mondays.append(next_date.date())\n",
    "    \n",
    "    # Assign each day in the dataset to a week index. To be assigned to a week index,\n",
    "    # date() >= mondays[i] & date() < mondays[i + 1]\n",
    "    week_assignments = []\n",
    "    cur_week = 0\n",
    "    for i, row in stock_df.iterrows():\n",
    "        row_date = row['timestamp'].date()\n",
    "        if row_date < mondays[cur_week]:\n",
    "            week_assignments.append(np.nan)\n",
    "            continue\n",
    "        j = 0\n",
    "        while row_date >= mondays[cur_week + j]:\n",
    "            j += 1\n",
    "            if cur_week + j == len(mondays):\n",
    "                break\n",
    "        week_assignments.append(cur_week + j - 1)\n",
    "\n",
    "        cur_week += (j - 1)\n",
    "    stock_df['week_index'] = week_assignments\n",
    "    return stock_df, first_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cbbcdf-22a6-444c-8c63-caee115966ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generates all combinations of weekday and week index. Uses the date of the first monday when the dataset\n",
    "## starts to compute a week number for each of the values in case the week number is missing in the actual dataset\n",
    "def get_index_day_combs(first_time, num_mondays):\n",
    "    tups = []\n",
    "    cur_day = first_time\n",
    "    for i in range(num_mondays):\n",
    "        for j in range(1, 6):\n",
    "            tups.append((i, j, datetime.datetime.strftime(cur_day, '%b')))\n",
    "            cur_day = cur_day + relativedelta(days = 1)\n",
    "        cur_day = cur_day + relativedelta(days = 2) # Get to the next monday\n",
    "    all_df = pd.DataFrame(tups, columns=['week_index', 'weekday', 'start_time'])\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0489e514-1ca8-47f5-a26a-e0b802d2678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputes the given columns of the dataframe with the average of the most recent previous complete value\n",
    "## and the next complete value\n",
    "\n",
    "# Function assumes that if one of impute_cols has a missing value, all will have missing values\n",
    "def impute_with_neighbors(stock_df, impute_cols):\n",
    "    prev_close = len(stock_df.loc[~pd.isna(stock_df['close'])])\n",
    "    prev_non_missing = np.nan\n",
    "    col_locs = [stock_df.columns.get_loc(col) for col in impute_cols]\n",
    "    cur_loc = 0\n",
    "    for i, row in stock_df.iterrows():\n",
    "        average_of = []\n",
    "        if not pd.isna(stock_df.iloc[cur_loc, col_locs[0]]):\n",
    "            prev_non_missing = cur_loc\n",
    "            cur_loc += 1\n",
    "            continue\n",
    "        if not pd.isna(prev_non_missing):\n",
    "            average_of.append(prev_non_missing)\n",
    "        j = 1\n",
    "        while cur_loc + j < len(stock_df) and pd.isna(stock_df.iloc[cur_loc + j, col_locs[0]]):\n",
    "            j += 1\n",
    "        if cur_loc + j != len(stock_df):\n",
    "            average_of.append(cur_loc + j)\n",
    "        if len(average_of) == 0:\n",
    "            raise ValueError('empty dataset')\n",
    "        # Actully insert the new values\n",
    "        for col_loc in col_locs:\n",
    "            stock_df.iloc[cur_loc, col_loc] = stock_df.iloc[average_of, col_loc].mean()\n",
    "        cur_loc += 1\n",
    "    new_close = len(stock_df.loc[~pd.isna(stock_df['close'])])\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf52e249-74b0-4ca4-81af-5d9047df434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training examples with each input containig 8 weeks worth of closing price and volume,\n",
    "## along with 12 indicator variables indicating the month of the year. The response variable\n",
    "## will be the percent change in price over the following week. This will be measured as closing\n",
    "## price from the last day in the data set (friday) to closing price on the following friday.\n",
    "## A training example will be created from every week in the dataset\n",
    "\n",
    "def create_examples(imputed_df, input_cols):\n",
    "    input_weeks = 8\n",
    "    change_after = 1\n",
    "    max_week = imputed_df['week_index'].max()\n",
    "    imputed_df = imputed_df.sort_values(['week_index', 'weekday'])\n",
    "    start_time_col = imputed_df.columns.get_loc('start_time')\n",
    "    \n",
    "    inputs = []\n",
    "    if max_week < 100:\n",
    "        raise ValueError(\"Not Enough Examples\")\n",
    "    queries = 0\n",
    "    stacking = 0\n",
    "    cleaning = 0\n",
    "    \n",
    "    for i in range(max_week - (input_weeks - 1 + change_after)):\n",
    "        st = datetime.datetime.now()\n",
    "        input_time = imputed_df.loc[imputed_df['week_index'] == i].iloc[0, start_time_col]\n",
    "        target_day = imputed_df.loc[(imputed_df['week_index'] == i + (input_weeks - 1) + change_after) & (imputed_df['weekday'] == 5)]\n",
    "        last_day = imputed_df.loc[(imputed_df['week_index'] == i + (input_weeks - 1)) & (imputed_df['weekday'] == 5)]\n",
    "        response = target_day.iloc[0, target_day.columns.get_loc('close')] / last_day.iloc[0, last_day.columns.get_loc('close')]\n",
    "        input_df = imputed_df.loc[(imputed_df['week_index'] >= i) & (imputed_df['week_index'] < i + input_weeks)]\n",
    "        en = datetime.datetime.now()\n",
    "        queries += (en - st).total_seconds()\n",
    "        st = datetime.datetime.now()\n",
    "        input_df = input_df.set_index(['week_index', 'weekday'])[input_cols]\n",
    "        input_stack = input_df.stack(dropna=False)\n",
    "        input_stack.index.names = ['week_index', 'weekday', 'metric']\n",
    "        input_stack.name = 'value'\n",
    "        input_df = input_stack.reset_index()\n",
    "        input_df['index'] = input_df.apply(lambda x: 'wi' + str(x['week_index'] - i) + '_wd' + str(x['weekday']) + '_' + x['metric'], axis=1)\n",
    "        input_df = input_df.set_index('index')\n",
    "        day_series = input_df['value']\n",
    "        en = datetime.datetime.now()\n",
    "        stacking += (en - st).total_seconds()\n",
    "        \n",
    "        st = datetime.datetime.now()\n",
    "        final_series = pd.concat([day_series, pd.Series([input_time, response], index=['start_time', 'week_change'])])\n",
    "        final_series.name=i\n",
    "        inputs.append(final_series)\n",
    "        en = datetime.datetime.now()\n",
    "        cleaning += (en - st).total_seconds()\n",
    "        \n",
    "    ret_df = pd.DataFrame(inputs)\n",
    "    print('queries', queries)\n",
    "    print('stacking', stacking)\n",
    "    print('cleaning', cleaning)\n",
    "    ret_df = to_one_hot(ret_df, 'start_time')\n",
    "    return ret_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c3e7c71-a644-45e4-866a-bcd48db2c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function assumes that the dataframe is in ascending order in terms of date\n",
    "# and that the frequency is 'day' - each row represents one day closing price\n",
    "def produce_ind_and_response(stock_df):\n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Assing a week index to each of the rows in the dataset\n",
    "    with_week_index, first_time = assign_week_index(stock_df)\n",
    "    en = datetime.datetime.now()\n",
    "    wi = (en - st).total_seconds()\n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Generate all possible combinations of week index and day of week\n",
    "    all_df = get_index_day_combs(first_time, int(with_week_index['week_index'].max()))\n",
    "    en = datetime.datetime.now()\n",
    "    al = (en - st).total_seconds()\n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Join the dataframes to locate missing values\n",
    "    with_missing = pd.merge(all_df, stock_df, how='left', on=['week_index', 'weekday']).reset_index(drop=True)\n",
    "    en = datetime.datetime.now()\n",
    "    wm = (en - st).total_seconds()\n",
    "    \n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Drop the last week of data as it is often incomplete\n",
    "    with_missing_truncated = with_missing.drop(with_missing.loc[with_missing['week_index'] == with_missing['week_index'].max()].index)\n",
    "    en = datetime.datetime.now()\n",
    "    wmt = (en - st).total_seconds()\n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Impute missing values with the mean of the last non-null value and the next non-null value\n",
    "    imputed_df = impute_with_neighbors(with_missing_truncated, ['close', 'volume'])\n",
    "    en = datetime.datetime.now()\n",
    "    idf = (en - st).total_seconds()\n",
    "    \n",
    "    st = datetime.datetime.now()\n",
    "    ## Create the actual training examples\n",
    "    formatted_df = create_examples(imputed_df, ['close', 'volume'])\n",
    "    en = datetime.datetime.now()\n",
    "    fdf = (en - st).total_seconds()\n",
    "                        \n",
    "    print('week index', wi)\n",
    "    print('all_df', al)\n",
    "    print('with_missing', wm)\n",
    "    print('truncated', wmt)\n",
    "    print('imputed', idf)\n",
    "    print('formatted', fdf)\n",
    "    return formatted_df\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a7ec01-8b0d-47b8-9850-0a4fdb1b069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries 0.6334279999999989\n",
      "stacking 1.7217699999999996\n",
      "cleaning 0.10707300000000011\n",
      "week index 0.112905\n",
      "all_df 0.048827\n",
      "with_missing 0.003341\n",
      "truncated 0.001045\n",
      "imputed 0.172918\n",
      "formatted 2.505165\n"
     ]
    }
   ],
   "source": [
    "df = produce_ind_and_response(stock_dfs['GOOGL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af12a3-e9ab-4d0b-a6d6-a0d76cf7c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns a number of metrics given predicted and actual values\n",
    "def get_metrics(predicted, actual):\n",
    "    # MSE\n",
    "    np_pred = np.array(predicted)\n",
    "    np_act = np.array(actual)\n",
    "    mse = ((np_pred - np_act) ** 2).mean()\n",
    "    acc = (np.maximum(0, 1 - np.absolute(np_pred - np_act) / np_act)).mean()\n",
    "    hl_acc = (((np_pred > 1) & (np_act > 1)) | ((np_pred < 1) & (np_act < 1))).mean()\n",
    "    return {'Mean Squared': mse, 'Accuracy': acc, 'High Low Accuracy': hl_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a25539-42eb-491d-9b0f-df1639398d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display Training and testing metrics using the get_metrics function above\n",
    "def display_metrics(train_y_pred, train_act, test_y_pred, test_act):\n",
    "    met_dic = get_metrics(train_y_pred, train_act)\n",
    "    keys = []\n",
    "    items = []\n",
    "    for key, item in met_dic.items():\n",
    "        keys.append(key)\n",
    "        items.append(items)\n",
    "    train_series = pd.Series(items, index=keys, name='Train')\n",
    "    \n",
    "    met_dic = get_metrics(test_y_pred, test_act)\n",
    "    keys = []\n",
    "    items = []\n",
    "    for key, item in met_dic.items():\n",
    "        keys.append(key)\n",
    "        items.append(items)\n",
    "    test_series = pd.Series(items, index=keys, name='Test')\n",
    "        \n",
    "    print(pd.DataFrame([train_series, test_series]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0277613-9d90-49a1-9f8d-49c2977919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runs k-fold cross validation\n",
    "## Assumes all variables other than the response are\n",
    "## predictors\n",
    "def cross_validate(k, in_df, response_var, model_instantiate):\n",
    "    shuffled = in_df.sample(frac=1)\n",
    "    test_ind = [int(i) for i in np.floor(np.linspace(0, len(shuffled), k + 1))]\n",
    "    metrics = {}\n",
    "    for i in range(k):\n",
    "        train = pd.concat((in_df.iloc[0: test_ind[i]], in_df.iloc[test_ind[i + 1]:]))\n",
    "        test = in_df.iloc[test_ind[i]:test_ind[i + 1]]\n",
    "        train_y = train[response_var]\n",
    "        train_x = train.drop(columns=[response_var], axis=1).copy()\n",
    "        test_y = test[response_var]\n",
    "        test_x = test.drop(columns=[response_var], axis=1).copy()\n",
    "        \n",
    "        model = eval(model_instantiate)\n",
    "        model.fit(train_x, train_y)\n",
    "        tr_pred = model.predict(train_x)\n",
    "        te_pred = model.predict(test_x)\n",
    "        \n",
    "        metric_dic = get_metrics(tr_pred, train_y)\n",
    "        for key, value in metric_dic.items():\n",
    "            if key not in metrics:\n",
    "                metrics[key] = [0, 0]\n",
    "            metrics[key][0] += value\n",
    "            \n",
    "        metric_dic = get_metrics(te_pred, test_y)\n",
    "        for key, value in metric_dic.items():\n",
    "            metrics[key][1] += value\n",
    "    df = pd.DataFrame(metrics, index=pd.Series(['Train', 'Test'], name='Phase'))\n",
    "    df = df.transform(lambda x: x / k)\n",
    "    return df       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd5d2f-1613-409d-a46e-add51ad8035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performs validation on one fold of the testing set and graphs the results\n",
    "def hl_acc_visual(in_df, response_var, model_instantiate):\n",
    "    y = in_df[response_var]\n",
    "    x = in_df.drop(columns=response_var, axis=1)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "    model = eval(model_instantiate)\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    hl = ((pred > 1) & (test_y > 1)) | ((pred < 1) & (test_y < 1))\n",
    "    bins = pd.cut(pred, 5)\n",
    "    return hl.groupby(bins).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570752a-cbe1-4e2f-8380-1170faac2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains Random Forest Regression Models\n",
    "dfs = []\n",
    "for symbol, stock_df in stock_dfs.items():\n",
    "    print('processing', symbol)\n",
    "    try:\n",
    "        res = produce_ind_and_response(stock_df)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    cv = cross_validate(10, res, 'week_change', \"RandomForestRegressor(n_estimators=20)\")\n",
    "    cv['Symbol'] = symbol\n",
    "    dfs.append(cv)\n",
    "\n",
    "results = pd.concat(dfs, axis=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0da2abbc-0e23-446f-9011-8283b4efaa3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not Enough Examples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/9htyqd7s7fvcqcn47w7nfyqc0000gn/T/ipykernel_50251/3985107893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduce_ind_and_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BBWI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#stock_dfs['CTRA']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5w/9htyqd7s7fvcqcn47w7nfyqc0000gn/T/ipykernel_50251/45284062.py\u001b[0m in \u001b[0;36mproduce_ind_and_response\u001b[0;34m(stock_df)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m## Create the actual training examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mformatted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'volume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5w/9htyqd7s7fvcqcn47w7nfyqc0000gn/T/ipykernel_50251/458880833.py\u001b[0m in \u001b[0;36mcreate_examples\u001b[0;34m(imputed_df, input_cols)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_week\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not Enough Examples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_week\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_weeks\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchange_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not Enough Examples"
     ]
    }
   ],
   "source": [
    "produce_ind_and_response(stock_dfs['BBWI'])\n",
    "#stock_dfs['CTRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10c950-0c5f-42fa-adfc-4ce9f6bc22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Explainable Boosting Regressor')\n",
    "cross_validate(10, res, 'week_change', \"ExplainableBoostingRegressor()\")\n",
    "print('Linear Regression')\n",
    "cross_validate(10, res, 'week_change', \"LinearRegression()\")\n",
    "print('Random Forest')\n",
    "cross_validate(10, res, 'week_change', \"RandomForestRegressor(n_estimators=20)\")\n",
    "print(hl_acc_visual(res, 'week_change', \"RandomForestRegressor(n_estimators=20)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51403009-fa6e-4412-859c-7a798b9fb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(['a', 'b', 'c'], index=[1, 1, 2], name='alpha')\n",
    "t = pd.Series(['a', 'b', 'c'], index=[1, 1, 2], name='alpha')\n",
    "\n",
    "df = pd.concat((s, t), axis=1)\n",
    "df['alpha'] = 1\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdfb09-59e4-407e-8440-776798b3bf98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBot",
   "language": "python",
   "name": "tbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
